# âœ¨ æ–‡ä»¶åå»ºè®®ï¼štrain_blip_with_imitation.py

import os
import glob
import time
from datetime import datetime
import random
import PIL
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import gym
from torch.utils.data import Dataset, DataLoader
from ppo_blip import PPO, PromptGenerationEnv
import argparse
import json
class ExpertDataset(Dataset):
    def __init__(self, expert_data):
        self.expert_data = expert_data

    def __len__(self):
        return len(self.expert_data)

    def __getitem__(self, idx):
        return self.expert_data[idx]

# collate_fn ä¸éœ€è¦ä¿®æ”¹
def collate_fn(batch, processor):
    initial_states, expert_prompts = zip(*batch)
    padding_value = processor.tokenizer.pad_token_id if processor.tokenizer.pad_token_id is not None else 0
    
    padded_prompts = nn.utils.rnn.pad_sequence(
        expert_prompts, batch_first=True, padding_value=padding_value
    )
    
    return list(initial_states), padded_prompts

def generate_sa_pairs_from_experts(image_path, num_prompts, blip_model, processor, env, pretrain_max_tokens, device):
    """
    âœ¨ [V6 - æœ€ç»ˆä¸€è‡´ç‰ˆ]
    é€šè¿‡åœ¨å¾ªç¯ä¸­æ‰‹åŠ¨è°ƒç”¨å‰å‘ä¼ æ’­æ¥ç”Ÿæˆ (çŠ¶æ€, åŠ¨ä½œ) å¯¹ã€‚
    æ­¤å‡½æ•°ä¸­çš„ h_t ç”Ÿæˆé€»è¾‘ç°åœ¨ä¸ ActorCritic._forward_model ä¸­çš„é€»è¾‘å®Œå…¨ä¸€è‡´ã€‚
    """
    print(f"--- Generating {num_prompts} expert trajectories using BLIP to create (state, action) pairs ---")
    sa_pairs = []
    
    try:
        image = PIL.Image.open(image_path).convert("RGB")
    except Exception as e:
        print(f"Error loading image {image_path}: {e}")
        return []

    for i in range(num_prompts):
        with torch.no_grad():
            # 1. é¦–å…ˆï¼Œç”¨ .generate() ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ä¸“å®¶åŠ¨ä½œåºåˆ— (expert_prompt_tokens)
            gen_inputs = processor(images=image, return_tensors="pt").to(device)
            generated_ids = blip_model.generate(
                **gen_inputs, 
                max_new_tokens=pretrain_max_tokens, 
                do_sample=True, 
                temperature=1.0
            )
            expert_prompt_tokens = generated_ids[0, 1:] # å»æ‰å¼€å¤´çš„ BOS token

            generated_text = processor.decode(expert_prompt_tokens, skip_special_tokens=True)
            print(f"     Processing expert prompt {i+1}/{num_prompts}: '{generated_text}'")

            # 2. ç„¶åï¼Œåœ¨å¾ªç¯ä¸­æ‰‹åŠ¨é‡æ¼”è¿™ä¸ªè¿‡ç¨‹ï¼Œä»¥è·å–ä¸PPOè®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„ h_t
            current_prompt_tokens = []
            for token_tensor in expert_prompt_tokens:
                # å‡†å¤‡å½“å‰æ—¶é—´æ­¥çš„è¾“å…¥
                step_inputs = processor(
                    images=image, 
                    text=processor.decode(current_prompt_tokens),
                    return_tensors="pt"
                ).to(device)

                # âœ¨âœ¨âœ¨ ä»¥ä¸‹ä»£ç å—ä¸ ActorCritic._forward_model ä¸­çš„é€»è¾‘å®Œå…¨ç›¸åŒ âœ¨âœ¨âœ¨
                # a. è·å–å›¾åƒç¼–ç 
                vision_outputs = blip_model.vision_model(pixel_values=step_inputs['pixel_values'])
                image_embeds = vision_outputs[0]
                image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)

                # b. è°ƒç”¨æ–‡æœ¬è§£ç å™¨
                decoder_outputs = blip_model.text_decoder(
                    input_ids=step_inputs['input_ids'],
                    attention_mask=step_inputs['attention_mask'],
                    encoder_hidden_states=image_embeds,
                    encoder_attention_mask=image_attention_mask,
                    output_hidden_states=True
                )
                
                # c. ä» .hidden_states å…ƒç»„ä¸­è·å–æœ€åä¸€å±‚çš„çŠ¶æ€
                last_hidden_states = decoder_outputs.hidden_states[-1]

                # d. æå–åºåˆ—æœ€åä¸€ä¸ªtokençš„éšè—çŠ¶æ€ä½œä¸º h_t
                h_t = last_hidden_states[:, -1, :].detach()
                # âœ¨âœ¨âœ¨ ä»¥ä¸Šä»£ç å—ä¸ ActorCritic._forward_model ä¸­çš„é€»è¾‘å®Œå…¨ç›¸åŒ âœ¨âœ¨âœ¨

                # åŠ¨ä½œæ˜¯ä¸“å®¶åºåˆ—ä¸­çš„å½“å‰ token
                next_token = token_tensor.to(device, dtype=torch.long)
                
                # è®°å½• (çŠ¶æ€, åŠ¨ä½œ) å¯¹
                sa_pairs.append((h_t, next_token))
                
                # æ›´æ–°ä¸Šä¸‹æ–‡ä»¥ç”¨äºä¸‹ä¸€æ­¥
                current_prompt_tokens.append(token_tensor.item())
                
    print(f"--- (State, Action) pair generation finished. Total pairs: {len(sa_pairs)} ---")
    return sa_pairs

def test_pretrained_policy(agent, env, max_len):
    print("\n--- Testing Pre-trained Policy ---")
    agent.policy_old.eval()
    state = env.reset()
    generated_tokens = []
    with torch.no_grad():
        for _ in range(max_len):
            action, _, _ = agent.policy_old.act(state)
            state['prompt'].append(action)
            
            # âœ¨ ä½¿ç”¨æ­£ç¡®çš„ EOS token id
            if action == agent.policy_old.processor.tokenizer.sep_token_id:
                break
            generated_tokens.append(action) # åªæ·»åŠ æœ‰æ•ˆ token

    prompt_text = agent.policy.processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)
    print(f"Generated Prompt: '{prompt_text}'")
    agent.policy_old.train()
    print("--- Testing Finished ---")


# ============================================================================================
# ä¸»è®­ç»ƒå‡½æ•°
# ============================================================================================
def train():
    parser = argparse.ArgumentParser(description='RL Fine-tuning of BLIP for Image Prompt Generation')
    print("============================================================================================")
    parser.add_argument('--image_dir', type=str, default="/home/mingzhel_umass_edu/inverse/LatentTracer/data/flickr30k/004.png", help='Path to the target image file.')
    parser.add_argument('--work_dir', type=str, default="./results", help='Path to the workplace.')
    parser.add_argument('--target_model_path', type=str, default="/project/pi_shiqingma_umass_edu/mingzheli/model/stable-diffusion-v1-5", help='Path to the workplace.')
 
    args = parser.parse_args()
    ####### è¶…å‚æ•°è®¾ç½® (ä¸åŸè„šæœ¬ä¿æŒä¸€è‡´) #######
    env_name = "PromptGenerationEnv"
    max_ep_len = 30
    max_training_timesteps = int(5000)
    print_freq = max_ep_len * 10
    log_freq = max_ep_len * 10
    save_model_freq = int(1e4)
    update_timestep = max_ep_len * 5
    K_epochs = 4
    eps_clip = 0.2
    gamma = 0.98
    lr_actor = 1e-4
    lr_critic = 5e-4
    random_seed = 0
    log_dir = "PPO_logs_BLIP"
    os.makedirs(log_dir, exist_ok=True)
    image_dir = args.image_dir

    
    # æ¨¡ä»¿å­¦ä¹ è¶…å‚æ•°
    pretrain_epochs = 2000
    pretrain_lr = 3e-4
    pretrain_batch_size = 8
    num_expert_prompts = 10
    pretrain_max_tokens = 20
    
    pretrain_checkpoint_path = f"PPO_preTrained_BLIP/PromptGenerationEnv/imitation_pretrained.pth"
    checkpoint_path = f"PPO_preTrained_BLIP/PromptGenerationEnv/PPO_{env_name}_{random_seed}_{image_dir[:-4]}.pth"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    env = PromptGenerationEnv(diffusion_model_name=args.target_model_path, image_dir=image_dir, max_prompt_length=max_ep_len)
    print(f"PPO Hyperparameters â lr_actor: {lr_actor}, lr_critic: {lr_critic}, gamma: {gamma}, K_epochs: {K_epochs}, eps_clip: {eps_clip} | Pre-training Hyperparameters â epochs: {pretrain_epochs}, lr: {pretrain_lr}, batch_size: {pretrain_batch_size}, num_prompts: {num_expert_prompts}, max_tokens: {pretrain_max_tokens} | Config â image: {image_dir}")

    print("\n=============================== Starting Phase 1: Imitation Learning ===============================")

    # âœ¨ å®ä¾‹åŒ–åŸºäº BLIP çš„ PPO agent
    ppo_agent_pretrain = PPO(lr_actor=pretrain_lr, lr_critic=pretrain_lr, gamma=gamma, K_epochs=K_epochs, eps_clip=eps_clip)
    
    # å†»ç»“ MLLMï¼Œåªè®­ç»ƒ adapter
    for param in ppo_agent_pretrain.policy_old.mllm.parameters():
        param.requires_grad = False
    for param in ppo_agent_pretrain.policy_old.adapter_mlp.parameters():
        param.requires_grad = True

    blip_model_expert = ppo_agent_pretrain.policy_old.mllm
    processor_expert = ppo_agent_pretrain.policy_old.processor
    
    # ç”Ÿæˆ (h_t, next_token) æ•°æ®é›†
    sa_pairs_list = generate_sa_pairs_from_experts(image_dir, num_expert_prompts, blip_model_expert, processor_expert, env, pretrain_max_tokens, device)
    
    if not sa_pairs_list:
        print("Failed to generate expert data. Exiting.")
        return

    expert_dataset = ExpertDataset(sa_pairs_list)
    expert_dataloader = DataLoader(expert_dataset, batch_size=pretrain_batch_size, shuffle=True)

    optimizer = optim.Adam(ppo_agent_pretrain.policy_old.adapter_mlp.parameters(), lr=pretrain_lr)
    loss_fn = nn.CrossEntropyLoss()

    ppo_agent_pretrain.policy_old.train() # ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼
    for epoch in range(pretrain_epochs):
        epoch_loss = 0
        for h_t_batch, next_token_batch in expert_dataloader:
            optimizer.zero_grad()
            
            h_t_batch = h_t_batch.squeeze(1).to(device)
            next_token_batch = next_token_batch.to(device)
            
            # --- é«˜æ•ˆçš„ç›‘ç£å­¦ä¹ æ­¥éª¤ ---
            # 1. å°† h_t æ‰¹æ¬¡é€šè¿‡ adapter_mlp
            adapted_h_t = ppo_agent_pretrain.policy_old.adapter_mlp(h_t_batch)
            
            # âœ¨ æ ¸å¿ƒä¿®å¤: ä½¿ç”¨ä¸ ActorCritic ä¸­å®Œå…¨ç›¸åŒçš„æ­£ç¡®è·¯å¾„
            # 2. å°†ç»“æœé€šè¿‡æ­£ç¡®çš„è¾“å‡ºå±‚å¾—åˆ° logits
            logits = ppo_agent_pretrain.policy_old.mllm.text_decoder.cls.predictions.decoder(adapted_h_t)
            
            # 3. è®¡ç®—æŸå¤±
            loss = loss_fn(logits, next_token_batch)
            # 4. åå‘ä¼ æ’­å’Œä¼˜åŒ–
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()

        if (epoch + 1) % 10 == 0:
            print(f"Pre-train Epoch [{epoch+1}/{pretrain_epochs}], Average Loss: {epoch_loss/len(expert_dataloader):.4f}")

    os.makedirs(os.path.dirname(pretrain_checkpoint_path), exist_ok=True)
    ppo_agent_pretrain.save(pretrain_checkpoint_path)
    print(f"--- Imitation learning finished. Pre-trained model saved to {pretrain_checkpoint_path} ---")

    test_pretrained_policy(ppo_agent_pretrain, env, max_ep_len)

    print("\n--- Cleaning up VRAM before starting RL phase ---")
    del ppo_agent_pretrain, blip_model_expert, processor_expert, expert_dataset, expert_dataloader, optimizer, sa_pairs_list
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    print("--- VRAM cleaned up ---")

    print("\n============================== Starting Phase 2: RL Fine-tuning ==============================")
    
# âœ¨ Re-instantiate the PPO agent
    ppo_agent_rl = PPO(lr_actor=lr_actor, lr_critic=lr_critic, gamma=gamma, K_epochs=K_epochs, eps_clip=eps_clip)
    
    try:
        ppo_agent_rl.load(pretrain_checkpoint_path)
        print(f"--- Successfully loaded pre-trained model from {pretrain_checkpoint_path} for RL fine-tuning ---")
    except FileNotFoundError:
        print("--- No pre-trained model found. Starting RL training from scratch. ---")

    start_time = datetime.now().replace(microsecond=0)
    print("Started RL training at (GMT): ", start_time)
    


    # <<< NEW: Initialize variables to track the best reward and prompt >>>
    best_reward = -float('inf')
    best_prompt = ""

    time_step = 0
    i_episode = 0

    while time_step <= max_training_timesteps:
        state = env.reset()
        current_ep_reward = 0
        
        for t in range(1, max_ep_len + 1):
            action = ppo_agent_rl.select_action(state)
            state, reward, done, _ = env.step(action)
            
            ppo_agent_rl.buffer.rewards.append(reward)
            ppo_agent_rl.buffer.is_terminals.append(done)
            
            time_step += 1
            current_ep_reward += reward

            if time_step % update_timestep == 0:
                ppo_agent_rl.update()

            if done:
                break
        
        # This part that decodes the prompt is the same
        prompt_text = env.processor.tokenizer.decode(state["prompt"][len(env.initial_tokens):], skip_special_tokens=True)
        print(f"Episode: {i_episode + 1} \t Timestep: {time_step} \t Reward: {current_ep_reward:.4f} \t Prompt: '{prompt_text}'")

        # <<< NEW: Check if the current episode's reward is the best so far >>>
        if current_ep_reward > best_reward:
            best_reward = current_ep_reward
            best_prompt = prompt_text
            print(f"ğŸ‰ New best reward found! Reward: {best_reward:.4f}")

        i_episode += 1

    # <<< NEW: Save the best prompt to a JSON file after training is complete >>>
    # We assume the image path is available in your 'env' or 'args' object.
    # This code will try to find it.
    image_path = args.image_dir # Fallback path


    output_filename = os.path.join(args.work_dir, "best_prompts.json")
    results = {}
    
    # Load existing data to avoid overwriting results for other images
    if os.path.exists(output_filename):
        with open(output_filename, 'r') as f:
            try:
                results = json.load(f)
            except json.JSONDecodeError:
                results = {} # Start with a fresh dictionary if file is corrupted

    # Update the dictionary with the result from this run
    results[image_path] = {
        "best_prompt": best_prompt,
        "best_reward": round(best_reward, 4)
    }

    # Write the updated dictionary back to the file
    with open(output_filename, 'w') as f:
        json.dump(results, f, indent=4)
    print(f"--- Best prompt for '{image_path}' saved to {output_filename} ---")

    env.close()

    end_time = datetime.now().replace(microsecond=0)
    print("Total training time: ", end_time - start_time)
    print("============================================================================================")

if __name__ == '__main__':
    train()